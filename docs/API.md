# ğŸ”Œ LLM OS API ë¬¸ì„œ

ì´ ë¬¸ì„œëŠ” LLM OSì˜ ì£¼ìš” í´ë˜ìŠ¤ì™€ ë©”ì„œë“œë“¤ì— ëŒ€í•œ API ì°¸ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

- [í•µì‹¬ í´ë˜ìŠ¤](#í•µì‹¬-í´ë˜ìŠ¤)
- [ê´€ë¦¬ì í´ë˜ìŠ¤](#ê´€ë¦¬ì-í´ë˜ìŠ¤)
- [ë°ì´í„° ëª¨ë¸](#ë°ì´í„°-ëª¨ë¸)
- [ì¸í„°í˜ì´ìŠ¤](#ì¸í„°í˜ì´ìŠ¤)
- [ìœ í‹¸ë¦¬í‹°](#ìœ í‹¸ë¦¬í‹°)

## í•µì‹¬ í´ë˜ìŠ¤

### EnhancedLLMOSApp

ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from src.llmos.core.app import EnhancedLLMOSApp

app = EnhancedLLMOSApp()
app.run()
```

#### ë©”ì„œë“œ

- `run()`: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
- `cleanup()`: ë¦¬ì†ŒìŠ¤ ì •ë¦¬
- `get_app_info()`: ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´ ë°˜í™˜

## ê´€ë¦¬ì í´ë˜ìŠ¤

### SettingsManager

ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from src.llmos.managers.settings import SettingsManager

settings = SettingsManager()
```

#### ì£¼ìš” ë©”ì„œë“œ

```python
# ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸°
value = settings.get("key.path", default_value)

# ì„¤ì • ê°’ ì„¤ì •í•˜ê¸°
settings.set("key.path", value)

# API í‚¤ ì„¤ì •
settings.set_api_key("openai", "your-api-key")

# API í‚¤ ì¡´ì¬ í™•ì¸
has_key = settings.has_api_key(ModelProvider.OPENAI)

# ì„¤ì • ì´ˆê¸°í™”
settings.reset_to_defaults()

# ì„¤ì • ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°
exported = settings.export_settings()
settings.import_settings(data)
```

### ChatSessionManager

ì±„íŒ… ì„¸ì…˜ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from src.llmos.managers.chat_sessions import ChatSessionManager

chat_manager = ChatSessionManager("/path/to/sessions")
```

#### ì£¼ìš” ë©”ì„œë“œ

```python
# ìƒˆ ì„¸ì…˜ ìƒì„±
session = chat_manager.create_session("ì„¸ì…˜ ì œëª©")

# ì„¸ì…˜ ì¡°íšŒ
session = chat_manager.get_session(session_id)

# ì„¸ì…˜ ì—…ë°ì´íŠ¸
chat_manager.update_session(session)

# ì„¸ì…˜ ì‚­ì œ
chat_manager.delete_session(session_id)

# ëª¨ë“  ì„¸ì…˜ ì¡°íšŒ
sessions = chat_manager.get_all_sessions()

# ì„¸ì…˜ ê²€ìƒ‰
results = chat_manager.search_sessions("ê²€ìƒ‰ì–´")

# ì„¸ì…˜ í†µê³„
stats = chat_manager.get_session_statistics()

# ë¹ˆ ì„¸ì…˜ ì •ë¦¬
deleted_count = chat_manager.cleanup_empty_sessions()
```

### ArtifactManager

ì•„í‹°íŒ©íŠ¸ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from src.llmos.managers.artifacts import ArtifactManager
from src.llmos.models.enums import ArtifactType

artifact_manager = ArtifactManager("/path/to/artifacts")
```

#### ì£¼ìš” ë©”ì„œë“œ

```python
# ì•„í‹°íŒ©íŠ¸ ìƒì„±
artifact = artifact_manager.create(
    type_val=ArtifactType.TEXT,
    title="ì œëª©",
    content="ë‚´ìš©",
    tags=["íƒœê·¸1", "íƒœê·¸2"],
    metadata={"key": "value"}
)

# ì•„í‹°íŒ©íŠ¸ ì¡°íšŒ
artifact = artifact_manager.get(artifact_id)

# ì•„í‹°íŒ©íŠ¸ ì—…ë°ì´íŠ¸
artifact_manager.update(artifact_id, title="ìƒˆ ì œëª©")

# ì•„í‹°íŒ©íŠ¸ ì‚­ì œ
artifact_manager.delete(artifact_id)

# ì•„í‹°íŒ©íŠ¸ ê²€ìƒ‰
results = artifact_manager.search(
    query="ê²€ìƒ‰ì–´",
    type_val=ArtifactType.CODE,
    tags=["íƒœê·¸1"],
    limit=50
)

# ì•„í‹°íŒ©íŠ¸ í†µê³„
stats = artifact_manager.get_statistics()
```

### UsageTracker

í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•©ë‹ˆë‹¤.

```python
from src.llmos.managers.usage_tracker import UsageTracker

usage_tracker = UsageTracker("/path/to/usage_data")
```

#### ì£¼ìš” ë©”ì„œë“œ

```python
# ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì¶”ê°€
usage_tracker.add_usage(token_usage)

# ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰
today_usage = usage_tracker.get_today_usage_from_summary()

# ì „ì²´ ì‚¬ìš©ëŸ‰
total_usage = usage_tracker.get_total_usage_from_history()

# ì£¼ê°„ ì‚¬ìš©ëŸ‰
weekly_usage = usage_tracker.get_weekly_usage()

# ì›”ê°„ ì‚¬ìš©ëŸ‰
monthly_usage = usage_tracker.get_monthly_usage()

# ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰
model_usage = usage_tracker.get_usage_by_model(days=30)

# ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ
trends = usage_tracker.get_usage_trends(days=7)

# ì›” ì˜ˆìƒ ë¹„ìš©
estimated_cost = usage_tracker.estimate_monthly_cost()

# ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
cleaned = usage_tracker.cleanup_old_data(keep_days=90)
```

### EnhancedModelManager

AI ëª¨ë¸ì„ ê´€ë¦¬í•˜ê³  í˜¸ì¶œí•©ë‹ˆë‹¤.

```python
from src.llmos.managers.model_manager import EnhancedModelManager

model_manager = EnhancedModelManager(settings_manager, usage_tracker)
```

#### ì£¼ìš” ë©”ì„œë“œ

```python
# AI ì‘ë‹µ ìƒì„±
response, usage = model_manager.generate(
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
    provider_display_name="OpenAI",
    model_id_key="chatgpt-4o-latest",
    temperature=0.7,
    max_tokens=1000
)

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
for chunk, usage in model_manager.stream_generate(messages):
    print(chunk)

# ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì—…ì²´
providers = model_manager.get_available_providers()

# ì œê³µì—…ì²´ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
is_available = model_manager.is_provider_available(ModelProvider.OPENAI)

# ëª¨ë¸ ì •ë³´ ì¡°íšŒ
model_info = model_manager.get_model_info("OpenAI", "chatgpt-4o-latest")

# ì„¤ì • ê²€ì¦
validation = model_manager.validate_configuration()
```

## ë°ì´í„° ëª¨ë¸

### ChatSession

ì±„íŒ… ì„¸ì…˜ ë°ì´í„° ëª¨ë¸ì…ë‹ˆë‹¤.

```python
from src.llmos.models.data_models import ChatSession
from datetime import datetime

session = ChatSession(
    id="session-id",
    title="ì±„íŒ… ì œëª©",
    messages=[
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
    ],
    created_at=datetime.now(),
    updated_at=datetime.now(),
    metadata={}
)

# ë”•ì…”ë„ˆë¦¬ ë³€í™˜
session_dict = session.to_dict()

# ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±
session = ChatSession.from_dict(session_dict)
```

### Artifact

ì•„í‹°íŒ©íŠ¸ ë°ì´í„° ëª¨ë¸ì…ë‹ˆë‹¤.

```python
from src.llmos.models.data_models import Artifact
from src.llmos.models.enums import ArtifactType
from datetime import datetime

artifact = Artifact(
    id="artifact-id",
    type=ArtifactType.TEXT,
    title="ì•„í‹°íŒ©íŠ¸ ì œëª©",
    content="ë‚´ìš©",
    tags=["íƒœê·¸1", "íƒœê·¸2"],
    created_at=datetime.now(),
    updated_at=datetime.now(),
    metadata={"key": "value"}
)
```

### TokenUsage

í† í° ì‚¬ìš©ëŸ‰ ë°ì´í„° ëª¨ë¸ì…ë‹ˆë‹¤.

```python
from src.llmos.models.data_models import TokenUsage
from datetime import datetime

usage = TokenUsage(
    input_tokens=100,
    output_tokens=50,
    total_tokens=150,
    model_name="gpt-4o",
    provider="openai",
    timestamp=datetime.now(),
    cost_usd=0.002
)
```

### ModelConfig

ëª¨ë¸ ì„¤ì • ë°ì´í„° ëª¨ë¸ì…ë‹ˆë‹¤.

```python
from src.llmos.models.data_models import ModelConfig
from src.llmos.models.enums import ModelProvider

config = ModelConfig(
    provider=ModelProvider.OPENAI,
    model_name="gpt-4o",
    display_name="GPT-4o",
    max_tokens=128000,
    supports_streaming=True,
    supports_functions=True,
    supports_vision=True,
    description="OpenAIì˜ ìµœì‹  ëª¨ë¸",
    input_cost_per_1k=0.005,
    output_cost_per_1k=0.015
)
```

## ì¸í„°í˜ì´ìŠ¤

### LLMInterface

AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from src.llmos.interfaces.base import LLMInterface

class CustomInterface(LLMInterface):
    def generate(self, messages, model, **kwargs):
        # êµ¬í˜„
        pass
    
    def stream(self, messages, model, **kwargs):
        # êµ¬í˜„
        pass
```

### OpenAIInterface

OpenAI API ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

```python
from src.llmos.interfaces.openai_client import OpenAIInterface

interface = OpenAIInterface("your-api-key")

# ì‘ë‹µ ìƒì„±
response, usage = interface.generate(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4o",
    temperature=0.7,
    max_tokens=100
)

# ìŠ¤íŠ¸ë¦¬ë°
for chunk in interface.stream(messages, model="gpt-4o"):
    print(chunk)
```

### AnthropicInterface

Anthropic Claude API ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

```python
from src.llmos.interfaces.anthropic_client import AnthropicInterface

interface = AnthropicInterface("your-api-key")

# ì‚¬ìš©ë²•ì€ OpenAIInterfaceì™€ ë™ì¼
```

### GoogleInterface

Google Gemini API ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

```python
from src.llmos.interfaces.google_client import GoogleInterface

interface = GoogleInterface("your-api-key")

# ì‚¬ìš©ë²•ì€ ë‹¤ë¥¸ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼
```

## ìœ í‹¸ë¦¬í‹°

### ë¡œê¹…

```python
from src.llmos.utils.logging_handler import setup_logging, get_app_logger

# ë¡œê¹… ì„¤ì •
setup_logging()

# ë¡œê±° ê°€ì ¸ì˜¤ê¸°
logger = get_app_logger("module_name")

# ì‚¬ìš©
logger.info("ì •ë³´ ë©”ì‹œì§€")
logger.error("ì˜¤ë¥˜ ë©”ì‹œì§€")
```

### ì¶œë ¥ ë Œë”ë§

```python
from src.llmos.utils.output_renderer import OutputRenderer

renderer = OutputRenderer()

# í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬
processed_text = renderer.process_output(raw_text)

# ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë Œë”ë§
result = renderer.render_with_metadata(text, {"key": "value"})
```

### í—¬í¼ í•¨ìˆ˜ë“¤

```python
from src.llmos.utils.helpers import *

# ID ìƒì„±
id = generate_id()
short_id = generate_short_id()

# í…ìŠ¤íŠ¸ ì²˜ë¦¬
truncated = truncate_text("ê¸´ í…ìŠ¤íŠ¸", max_length=50)
safe_filename = sanitize_filename("íŒŒì¼ì´ë¦„.txt")

# íŒŒì¼ ì²˜ë¦¬
file_hash = calculate_file_hash("file.txt")
file_size = format_file_size(1024)

# ì´ë¯¸ì§€ ì²˜ë¦¬
mime_type = detect_image_mime_type(image_bytes)
resized_image = resize_image(image_bytes, max_width=800)
is_valid = validate_image(image_bytes)

# ë°ì´í„° URI
data_uri = create_data_uri(image_bytes, "image/png")
mime_type, data = parse_data_uri(data_uri)

# ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
cleaned = clean_dict({"a": None, "b": "value"}, remove_none=True)
merged = merge_dicts(dict1, dict2)
value = safe_get(data, "nested.key.path", default="default")

# ê¸°íƒ€
reading_time = estimate_reading_time(text)
urls = extract_urls(text)
masked = mask_sensitive_data(text)
```

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|--------|------|--------|------|
| `OPENAI_API_KEY` | string | - | OpenAI API í‚¤ |
| `ANTHROPIC_API_KEY` | string | - | Anthropic API í‚¤ |
| `GOOGLE_API_KEY` | string | - | Google AI API í‚¤ |
| `LLMOS_CONFIG_DIR` | string | `~/.llm_os_config` | ì„¤ì • ë””ë ‰í† ë¦¬ |
| `LLMOS_LOG_LEVEL` | string | `INFO` | ë¡œê·¸ ë ˆë²¨ |
| `LLMOS_DEBUG` | boolean | `false` | ë””ë²„ê·¸ ëª¨ë“œ |

### ì„¤ì • êµ¬ì¡°

```json
{
  "api_keys": {
    "openai": "your-openai-key",
    "anthropic": "your-anthropic-key",
    "google": "your-google-key"
  },
  "paths": {
    "chat_sessions": "~/.llm_os_config/chat_sessions",
    "artifacts": "~/.llm_os_config/artifacts",
    "usage_tracking": "~/.llm_os_config/usage_data"
  },
  "defaults": {
    "model": null,
    "temperature": 0.7,
    "max_tokens": 4096
  },
  "ui": {
    "selected_provider": null,
    "theme": "auto",
    "language": "ko"
  },
  "features": {
    "auto_title_generation": true,
    "usage_tracking": true,
    "debug_mode": false
  }
}
```

## ğŸ“ˆ í™•ì¥í•˜ê¸°

### ìƒˆ AI ì œê³µì—…ì²´ ì¶”ê°€

1. `LLMInterface`ë¥¼ ìƒì†í•˜ëŠ” ìƒˆ í´ë˜ìŠ¤ ìƒì„±
2. `ModelRegistry`ì— ëª¨ë¸ ì„¤ì • ì¶”ê°€
3. `ModelManager`ì—ì„œ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€

```python
from src.llmos.interfaces.base import LLMInterface

class NewProviderInterface(LLMInterface):
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def generate(self, messages, model, **kwargs):
        # êµ¬í˜„
        pass
    
    def stream(self, messages, model, **kwargs):
        # êµ¬í˜„  
        pass
```

### ìƒˆ ì•„í‹°íŒ©íŠ¸ íƒ€ì… ì¶”ê°€

```python
from src.llmos.models.enums import ArtifactType

# ìƒˆ íƒ€ì…ì„ enumì— ì¶”ê°€
class ArtifactType(Enum):
    # ê¸°ì¡´ íƒ€ì…ë“¤...
    NEW_TYPE = "new_type"
```

### ì»¤ìŠ¤í…€ UI ì»´í¬ë„ŒíŠ¸

```python
from src.llmos.ui.components import EnhancedUI

class CustomUI(EnhancedUI):
    @staticmethod
    def render_custom_component():
        # êµ¬í˜„
        pass
```

ì´ API ë¬¸ì„œëŠ” LLM OSì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ” ì†ŒìŠ¤ ì½”ë“œì˜ ë…ìŠ¤íŠ¸ë§ì„ ì°¸ì¡°í•˜ì„¸ìš”.